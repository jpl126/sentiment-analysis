{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words similarity analysis\n",
    "This notebook was an attempt to check if we could avoid deep learning and predict sentiment based on similarity of word vectors to either positive or negative ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "import re\n",
    "import string\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = pd.read_csv('/nlp/data/Tweets.csv')[['text', 'airline_sentiment']]\n",
    "messages = data_['text']\n",
    "sentiment = data_['airline_sentiment']\n",
    "# work only on a sample - it should be enough to find out which words are the most popular in the negative or positive tweets.\n",
    "data_ = data_.iloc[:2000]\n",
    "messages = messages.iloc[:2000]\n",
    "sentiment = sentiment.iloc[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing punctuation marks from phrases\n",
    "At the moment we don't bother with contraction and simply treat `aren't` as `aren` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VirginAmerica', 'What', 'dhepburn', 'said']\n",
      "['virginamerica', 'what', 'dhepburn', 'said']\n",
      "['VirginAmerica', 'plus', 'you', 've', 'added', 'commercials', 'to', 'the', 'experience', 'tacky']\n",
      "['virginamerica', 'plus', 'you', 've', 'added', 'commercials', 'to', 'the', 'experience', 'tacky']\n",
      "['VirginAmerica', 'I', 'didn', 't', 'today', 'Must', 'mean', 'I', 'need', 'to', 'take', 'another', 'trip']\n",
      "['virginamerica', 'i', 'didn', 't', 'today', 'must', 'mean', 'i', 'need', 'to', 'take', 'another', 'trip']\n"
     ]
    }
   ],
   "source": [
    "def remove_punctation_marks(msg: str) -> List[str]:\n",
    "    return list(filter(None, re.split(r'[\\W]', msg)))\n",
    "\n",
    "def lower_str_list(str_list: List[str]) -> List[str]:\n",
    "    return [w.lower() for w in str_list]\n",
    "\n",
    "# few examples\n",
    "for msg in messages[:3]:\n",
    "    print(remove_punctation_marks(msg))\n",
    "    print(lower_str_list(remove_punctation_marks(msg)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find most popular words in negative, positive, and neutral tweets\n",
    "I'll skip all stop words from the list from   [here](https://raw.githubusercontent.com/stanfordnlp/CoreNLP/master/data/edu/stanford/nlp/patterns/surface/stopwords.txt)\n",
    "\n",
    "They are far too popular to be inforamtive what sentiment the tweet has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/nlp/data/stopwords.txt') as f:\n",
    "    stop_words = f.read().splitlines()\n",
    "stop_words.remove('not') # we want to have negation words - can be helpful for negative sentiment\n",
    "stop_words.remove('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_.copy()\n",
    "data['airline_sentiment'] = data_['airline_sentiment'].replace({'negative': -1, 'neutral': 0, 'positive': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_popular_words_from_series(s: pd.Series, how_many=50, stop_words=[]) -> pd.Series:\n",
    "    all_words = []\n",
    "    for phrase in s:\n",
    "        all_words = chain(all_words, remove_punctation_marks(phrase))\n",
    "    all_words = lower_str_list(all_words)\n",
    "    result = pd.Series(all_words).value_counts()\n",
    "    result = result[~result.index.isin(stop_words)]\n",
    "    return result[:how_many]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The most popular words for given class\n",
    "Chose a few words which are the most popular for a given class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flight          2943\n",
       "united          2899\n",
       "usairways       2375\n",
       "t               2263\n",
       "americanair     2110\n",
       "not             1357\n",
       "no              1326\n",
       "southwestair    1214\n",
       "jetblue         1052\n",
       "get              988\n",
       "s                945\n",
       "cancelled        926\n",
       "now              833\n",
       "service          750\n",
       "2                735\n",
       "hours            649\n",
       "just             622\n",
       "help             619\n",
       "hold             614\n",
       "customer         614\n",
       "time             598\n",
       "plane            532\n",
       "m                521\n",
       "delayed          508\n",
       "amp              503\n",
       "still            492\n",
       "us               480\n",
       "call             462\n",
       "co               455\n",
       "hour             452\n",
       "flightled        448\n",
       "one              442\n",
       "http             437\n",
       "will             435\n",
       "bag              420\n",
       "flights          420\n",
       "gate             411\n",
       "ve               399\n",
       "don              388\n",
       "late             378\n",
       "back             375\n",
       "need             373\n",
       "phone            369\n",
       "3                347\n",
       "waiting          341\n",
       "please           328\n",
       "thanks           315\n",
       "4                303\n",
       "got              299\n",
       "airline          294\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# negative\n",
    "neg_df = data[data['airline_sentiment'] == -1]\n",
    "get_most_popular_words_from_series(neg_df['text'], stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_vectors = [\n",
    "    'cancelled', 'get', 'service', 'not', 'no', 'now', 'hour', 'hold', 'delayed', 'still',\n",
    "    'call', 'gate', 'late', 'bag', 'need', 'waiting', 'please', 'airline'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "thanks           611\n",
       "jetblue          595\n",
       "southwestair     576\n",
       "united           528\n",
       "thank            455\n",
       "flight           381\n",
       "t                355\n",
       "americanair      355\n",
       "usairways        276\n",
       "great            236\n",
       "co               233\n",
       "http             217\n",
       "s                208\n",
       "just             179\n",
       "service          162\n",
       "virginamerica    156\n",
       "love             136\n",
       "will             116\n",
       "customer         114\n",
       "get              114\n",
       "guys             110\n",
       "much             109\n",
       "good             109\n",
       "best             105\n",
       "awesome          100\n",
       "got              100\n",
       "time              97\n",
       "now               90\n",
       "us                87\n",
       "help              84\n",
       "today             83\n",
       "amp               82\n",
       "airline           81\n",
       "amazing           78\n",
       "not               78\n",
       "back              73\n",
       "m                 73\n",
       "flying            70\n",
       "crew              70\n",
       "gate              65\n",
       "fly               64\n",
       "re                63\n",
       "no                63\n",
       "made              63\n",
       "appreciate        62\n",
       "like              61\n",
       "ll                61\n",
       "please            60\n",
       "see               60\n",
       "flights           60\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# positive\n",
    "pos_df = data[data['airline_sentiment'] == 1]\n",
    "get_most_popular_words_from_series(pos_df['text'], stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_vectors = [\n",
    "    'thank', 'great', 'appreciate', 'love', 'guys', 'much', 'good', 'best', 'awesome', 'us', 'amazing'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jetblue          748\n",
       "united           737\n",
       "t                732\n",
       "southwestair     671\n",
       "flight           615\n",
       "co               526\n",
       "http             501\n",
       "americanair      499\n",
       "s                403\n",
       "usairways        402\n",
       "get              238\n",
       "will             217\n",
       "please           182\n",
       "virginamerica    177\n",
       "just             173\n",
       "help             170\n",
       "flights          169\n",
       "need             164\n",
       "thanks           157\n",
       "not              152\n",
       "dm               132\n",
       "m                130\n",
       "no               118\n",
       "2                115\n",
       "now              114\n",
       "tomorrow         109\n",
       "us               107\n",
       "fleek            107\n",
       "know             104\n",
       "fleet            103\n",
       "cancelled        101\n",
       "amp               98\n",
       "time              98\n",
       "way               94\n",
       "change            88\n",
       "one               80\n",
       "fly               79\n",
       "new               79\n",
       "flying            78\n",
       "back              78\n",
       "like              77\n",
       "today             77\n",
       "number            76\n",
       "check             74\n",
       "see               71\n",
       "go                71\n",
       "thank             69\n",
       "got               69\n",
       "travel            65\n",
       "ticket            64\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# neutral\n",
    "pos_df = data[data['airline_sentiment'] == 0]\n",
    "get_most_popular_words_from_series(pos_df['text'], stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is nothing explicitly characteristic so I will skip neutral for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load glove similarity model\n",
    "We don't want to work on the bag of words model due to its lack of similarity information between the words (e.g. word *hotel* is orthogonal to word *motel*).  \n",
    "The alternative to the bag of words is using word representation like the [GloVe](https://nlp.stanford.edu/projects/glove/).  \n",
    "It is encoding word into *n* dimensional vector (using PCA) which includes similarity of words based on its context (i.e. do they occur next to each other).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = datapath('/nlp/data/glove.6B.100d.txt')\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_similarity(model: KeyedVectors, word: str, vectors: List[str]) -> Union[float, None]:\n",
    "    \"\"\"\n",
    "    Predict average similarity betweeen a word and a vector of sentimental words.\n",
    "    \"\"\"\n",
    "    similarity = 0\n",
    "    for vec in vectors:\n",
    "        try:\n",
    "            similarity += model.similarity(word, vec)\n",
    "        except KeyError as e:\n",
    "            return None\n",
    "    return similarity / len(vectors)\n",
    "\n",
    "\n",
    "def get_pharse_average_similarity(\n",
    "        phrase: List[str], model: KeyedVectors, vectors: List[str]) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Get average positive and negative similarity\n",
    "    \"\"\"\n",
    "    vec_sim = 0\n",
    "    words_count = 0\n",
    "    for word in phrase:\n",
    "        if word not in model.vocab:\n",
    "            continue\n",
    "        vec_sim += get_average_similarity(model, word, vectors)\n",
    "        words_count += 1\n",
    "    if vec_sim == 0:\n",
    "        return np.nan\n",
    "    return vec_sim / words_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check similarity between class words and tweet text\n",
    "If similarity between the words in a tweet text and the most popular words in a class is sufficient enough to distinct the negative class from the positive one we could ommit the deep learning and save computing power for more sophisticated tasks ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>positive_similarity</th>\n",
       "      <th>negative_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.305686</td>\n",
       "      <td>0.305942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.289627</td>\n",
       "      <td>0.306438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.309066</td>\n",
       "      <td>0.321683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.290075</td>\n",
       "      <td>0.300603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.321671</td>\n",
       "      <td>0.322343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.284316</td>\n",
       "      <td>0.292499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.284237</td>\n",
       "      <td>0.286410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@VirginAmerica Really missed a prime opportuni...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.283590</td>\n",
       "      <td>0.295685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@virginamerica Well, I didn't‚Ä¶but NOW I DO! :-D</td>\n",
       "      <td>1</td>\n",
       "      <td>0.326240</td>\n",
       "      <td>0.319640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@VirginAmerica it was amazing, and arrived an ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.304602</td>\n",
       "      <td>0.309279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@VirginAmerica did you know that suicide is th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.290414</td>\n",
       "      <td>0.304190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@VirginAmerica I &amp;lt;3 pretty graphics. so muc...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.292197</td>\n",
       "      <td>0.300392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>@VirginAmerica This is such a great deal! Alre...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.289814</td>\n",
       "      <td>0.298433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@VirginAmerica @virginmedia I'm flying your #f...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.289037</td>\n",
       "      <td>0.295315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@VirginAmerica Thanks!</td>\n",
       "      <td>1</td>\n",
       "      <td>0.325688</td>\n",
       "      <td>0.311782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@VirginAmerica SFO-PDX schedule is still MIA.</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.317380</td>\n",
       "      <td>0.316756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>@VirginAmerica So excited for my first cross c...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.286052</td>\n",
       "      <td>0.291979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>@VirginAmerica  I flew from NYC to SFO last we...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.274213</td>\n",
       "      <td>0.284558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I ‚ù§Ô∏è flying @VirginAmerica. ‚ò∫Ô∏èüëç</td>\n",
       "      <td>1</td>\n",
       "      <td>0.315454</td>\n",
       "      <td>0.299148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@VirginAmerica you know what would be amazingl...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.288684</td>\n",
       "      <td>0.280248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>@VirginAmerica why are your first fares in May...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.291754</td>\n",
       "      <td>0.303529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>@VirginAmerica I love this graphic. http://t.c...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.303809</td>\n",
       "      <td>0.312759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>@VirginAmerica I love the hipster innovation. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.294868</td>\n",
       "      <td>0.302926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>@VirginAmerica will you be making BOS&amp;gt;LAS n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.279047</td>\n",
       "      <td>0.280715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>@VirginAmerica you guys messed up my seating.....</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.281180</td>\n",
       "      <td>0.295553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>@VirginAmerica status match program.  I applie...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.292502</td>\n",
       "      <td>0.310569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>@VirginAmerica What happened 2 ur vegan food o...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.285013</td>\n",
       "      <td>0.297333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>@VirginAmerica do you miss me? Don't worry we'...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.271480</td>\n",
       "      <td>0.271481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>@VirginAmerica amazing to me that we can't get...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.283518</td>\n",
       "      <td>0.296442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>@VirginAmerica LAX to EWR - Middle seat on a r...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.288908</td>\n",
       "      <td>0.304503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14610</th>\n",
       "      <td>@AmericanAir I understand the weather issue bu...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.281085</td>\n",
       "      <td>0.299623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14611</th>\n",
       "      <td>@AmericanAir guarantee no retribution? If so, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.301118</td>\n",
       "      <td>0.311355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14612</th>\n",
       "      <td>@AmericanAir a friend is having flight Cancell...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.271152</td>\n",
       "      <td>0.283808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14613</th>\n",
       "      <td>@AmericanAir I used the \"call back\" feature wi...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.273927</td>\n",
       "      <td>0.288654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14614</th>\n",
       "      <td>@AmericanAir I need to be at work tomorrow at ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.274996</td>\n",
       "      <td>0.296327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14615</th>\n",
       "      <td>@AmericanAir  ugh Dump us in dfw w/no luggage ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.275587</td>\n",
       "      <td>0.281296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14616</th>\n",
       "      <td>@AmericanAir Cancelled Flights my flight, does...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.284350</td>\n",
       "      <td>0.295387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14617</th>\n",
       "      <td>@AmericanAir DMing you now! Big thanks.</td>\n",
       "      <td>1</td>\n",
       "      <td>0.304368</td>\n",
       "      <td>0.294101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14618</th>\n",
       "      <td>@AmericanAir 3078 is overweight so you pull 2 ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.271581</td>\n",
       "      <td>0.276462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14619</th>\n",
       "      <td>@AmericanAir I love your company and your staf...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.272530</td>\n",
       "      <td>0.285651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14620</th>\n",
       "      <td>@AmericanAir I wait 2+ hrs for CS to call me b...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.269829</td>\n",
       "      <td>0.283525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14621</th>\n",
       "      <td>@AmericanAir I've been on hold for 55 mins abo...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.276364</td>\n",
       "      <td>0.293320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14622</th>\n",
       "      <td>I just need a place to sleep when I land witho...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.280449</td>\n",
       "      <td>0.293359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14623</th>\n",
       "      <td>@AmericanAir Love the new planes for the JFK-L...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.267504</td>\n",
       "      <td>0.282883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14624</th>\n",
       "      <td>@AmericanAir Call me Chairman, or call me Emer...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.274069</td>\n",
       "      <td>0.290721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14625</th>\n",
       "      <td>@AmericanAir Flight 236 was great. Fantastic c...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.291963</td>\n",
       "      <td>0.312175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14626</th>\n",
       "      <td>@AmericanAir Flight 953 NYC-Buenos Aires has b...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.292302</td>\n",
       "      <td>0.310562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14627</th>\n",
       "      <td>@AmericanAir Flight Cancelled Flightled, can't...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.289877</td>\n",
       "      <td>0.295609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14628</th>\n",
       "      <td>Thank you. ‚Äú@AmericanAir: @jlhalldc Customer R...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.267037</td>\n",
       "      <td>0.278334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14629</th>\n",
       "      <td>@AmericanAir How do I change my flight if the ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.269367</td>\n",
       "      <td>0.283853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14630</th>\n",
       "      <td>@AmericanAir Thanks! He is.</td>\n",
       "      <td>1</td>\n",
       "      <td>0.336434</td>\n",
       "      <td>0.333001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14631</th>\n",
       "      <td>@AmericanAir thx for nothing on getting us out...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.268663</td>\n",
       "      <td>0.282777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14632</th>\n",
       "      <td>‚Äú@AmericanAir: @TilleyMonsta George, that does...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.278543</td>\n",
       "      <td>0.294188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14633</th>\n",
       "      <td>@AmericanAir my flight was Cancelled Flightled...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.271588</td>\n",
       "      <td>0.280384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14634</th>\n",
       "      <td>@AmericanAir right on cue with the delaysüëå</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.281660</td>\n",
       "      <td>0.284587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>@AmericanAir thank you we got on a different f...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.283177</td>\n",
       "      <td>0.288276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.288979</td>\n",
       "      <td>0.300681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>@AmericanAir Please bring American Airlines to...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.290369</td>\n",
       "      <td>0.296954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>@AmericanAir you have my money, you change my ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.264050</td>\n",
       "      <td>0.271768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14639</th>\n",
       "      <td>@AmericanAir we have 8 ppl so we need 2 know h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.254780</td>\n",
       "      <td>0.274371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14640 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  airline_sentiment  \\\n",
       "0                    @VirginAmerica What @dhepburn said.                  0   \n",
       "1      @VirginAmerica plus you've added commercials t...                  1   \n",
       "2      @VirginAmerica I didn't today... Must mean I n...                  0   \n",
       "3      @VirginAmerica it's really aggressive to blast...                 -1   \n",
       "4      @VirginAmerica and it's a really big bad thing...                 -1   \n",
       "5      @VirginAmerica seriously would pay $30 a fligh...                 -1   \n",
       "6      @VirginAmerica yes, nearly every time I fly VX...                  1   \n",
       "7      @VirginAmerica Really missed a prime opportuni...                  0   \n",
       "8        @virginamerica Well, I didn't‚Ä¶but NOW I DO! :-D                  1   \n",
       "9      @VirginAmerica it was amazing, and arrived an ...                  1   \n",
       "10     @VirginAmerica did you know that suicide is th...                  0   \n",
       "11     @VirginAmerica I &lt;3 pretty graphics. so muc...                  1   \n",
       "12     @VirginAmerica This is such a great deal! Alre...                  1   \n",
       "13     @VirginAmerica @virginmedia I'm flying your #f...                  1   \n",
       "14                                @VirginAmerica Thanks!                  1   \n",
       "15         @VirginAmerica SFO-PDX schedule is still MIA.                 -1   \n",
       "16     @VirginAmerica So excited for my first cross c...                  1   \n",
       "17     @VirginAmerica  I flew from NYC to SFO last we...                 -1   \n",
       "18                       I ‚ù§Ô∏è flying @VirginAmerica. ‚ò∫Ô∏èüëç                  1   \n",
       "19     @VirginAmerica you know what would be amazingl...                  1   \n",
       "20     @VirginAmerica why are your first fares in May...                 -1   \n",
       "21     @VirginAmerica I love this graphic. http://t.c...                  1   \n",
       "22     @VirginAmerica I love the hipster innovation. ...                  1   \n",
       "23     @VirginAmerica will you be making BOS&gt;LAS n...                  0   \n",
       "24     @VirginAmerica you guys messed up my seating.....                 -1   \n",
       "25     @VirginAmerica status match program.  I applie...                 -1   \n",
       "26     @VirginAmerica What happened 2 ur vegan food o...                 -1   \n",
       "27     @VirginAmerica do you miss me? Don't worry we'...                  0   \n",
       "28     @VirginAmerica amazing to me that we can't get...                 -1   \n",
       "29     @VirginAmerica LAX to EWR - Middle seat on a r...                  0   \n",
       "...                                                  ...                ...   \n",
       "14610  @AmericanAir I understand the weather issue bu...                 -1   \n",
       "14611  @AmericanAir guarantee no retribution? If so, ...                  0   \n",
       "14612  @AmericanAir a friend is having flight Cancell...                 -1   \n",
       "14613  @AmericanAir I used the \"call back\" feature wi...                 -1   \n",
       "14614  @AmericanAir I need to be at work tomorrow at ...                 -1   \n",
       "14615  @AmericanAir  ugh Dump us in dfw w/no luggage ...                 -1   \n",
       "14616  @AmericanAir Cancelled Flights my flight, does...                 -1   \n",
       "14617            @AmericanAir DMing you now! Big thanks.                  1   \n",
       "14618  @AmericanAir 3078 is overweight so you pull 2 ...                 -1   \n",
       "14619  @AmericanAir I love your company and your staf...                  1   \n",
       "14620  @AmericanAir I wait 2+ hrs for CS to call me b...                 -1   \n",
       "14621  @AmericanAir I've been on hold for 55 mins abo...                 -1   \n",
       "14622  I just need a place to sleep when I land witho...                 -1   \n",
       "14623  @AmericanAir Love the new planes for the JFK-L...                  1   \n",
       "14624  @AmericanAir Call me Chairman, or call me Emer...                 -1   \n",
       "14625  @AmericanAir Flight 236 was great. Fantastic c...                  1   \n",
       "14626  @AmericanAir Flight 953 NYC-Buenos Aires has b...                 -1   \n",
       "14627  @AmericanAir Flight Cancelled Flightled, can't...                 -1   \n",
       "14628  Thank you. ‚Äú@AmericanAir: @jlhalldc Customer R...                  1   \n",
       "14629  @AmericanAir How do I change my flight if the ...                 -1   \n",
       "14630                        @AmericanAir Thanks! He is.                  1   \n",
       "14631  @AmericanAir thx for nothing on getting us out...                 -1   \n",
       "14632  ‚Äú@AmericanAir: @TilleyMonsta George, that does...                  0   \n",
       "14633  @AmericanAir my flight was Cancelled Flightled...                 -1   \n",
       "14634         @AmericanAir right on cue with the delaysüëå                 -1   \n",
       "14635  @AmericanAir thank you we got on a different f...                  1   \n",
       "14636  @AmericanAir leaving over 20 minutes Late Flig...                 -1   \n",
       "14637  @AmericanAir Please bring American Airlines to...                  0   \n",
       "14638  @AmericanAir you have my money, you change my ...                 -1   \n",
       "14639  @AmericanAir we have 8 ppl so we need 2 know h...                  0   \n",
       "\n",
       "       positive_similarity  negative_similarity  \n",
       "0                 0.305686             0.305942  \n",
       "1                 0.289627             0.306438  \n",
       "2                 0.309066             0.321683  \n",
       "3                 0.290075             0.300603  \n",
       "4                 0.321671             0.322343  \n",
       "5                 0.284316             0.292499  \n",
       "6                 0.284237             0.286410  \n",
       "7                 0.283590             0.295685  \n",
       "8                 0.326240             0.319640  \n",
       "9                 0.304602             0.309279  \n",
       "10                0.290414             0.304190  \n",
       "11                0.292197             0.300392  \n",
       "12                0.289814             0.298433  \n",
       "13                0.289037             0.295315  \n",
       "14                0.325688             0.311782  \n",
       "15                0.317380             0.316756  \n",
       "16                0.286052             0.291979  \n",
       "17                0.274213             0.284558  \n",
       "18                0.315454             0.299148  \n",
       "19                0.288684             0.280248  \n",
       "20                0.291754             0.303529  \n",
       "21                0.303809             0.312759  \n",
       "22                0.294868             0.302926  \n",
       "23                0.279047             0.280715  \n",
       "24                0.281180             0.295553  \n",
       "25                0.292502             0.310569  \n",
       "26                0.285013             0.297333  \n",
       "27                0.271480             0.271481  \n",
       "28                0.283518             0.296442  \n",
       "29                0.288908             0.304503  \n",
       "...                    ...                  ...  \n",
       "14610             0.281085             0.299623  \n",
       "14611             0.301118             0.311355  \n",
       "14612             0.271152             0.283808  \n",
       "14613             0.273927             0.288654  \n",
       "14614             0.274996             0.296327  \n",
       "14615             0.275587             0.281296  \n",
       "14616             0.284350             0.295387  \n",
       "14617             0.304368             0.294101  \n",
       "14618             0.271581             0.276462  \n",
       "14619             0.272530             0.285651  \n",
       "14620             0.269829             0.283525  \n",
       "14621             0.276364             0.293320  \n",
       "14622             0.280449             0.293359  \n",
       "14623             0.267504             0.282883  \n",
       "14624             0.274069             0.290721  \n",
       "14625             0.291963             0.312175  \n",
       "14626             0.292302             0.310562  \n",
       "14627             0.289877             0.295609  \n",
       "14628             0.267037             0.278334  \n",
       "14629             0.269367             0.283853  \n",
       "14630             0.336434             0.333001  \n",
       "14631             0.268663             0.282777  \n",
       "14632             0.278543             0.294188  \n",
       "14633             0.271588             0.280384  \n",
       "14634             0.281660             0.284587  \n",
       "14635             0.283177             0.288276  \n",
       "14636             0.288979             0.300681  \n",
       "14637             0.290369             0.296954  \n",
       "14638             0.264050             0.271768  \n",
       "14639             0.254780             0.274371  \n",
       "\n",
       "[14640 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_['positive_similarity'] = data_['text'].apply(get_pharse_average_similarity, args=(model, positive_vectors))\n",
    "data_['negative_similarity'] = data_['text'].apply(get_pharse_average_similarity, args=(model, negative_vectors))\n",
    "data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "Unfortunately the similarity to classes is ambiguous and we can see clearly that it is not enough to distinct correct class from the other ones.  \n",
    "**The next step is using deep learning and RNN for this task.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
